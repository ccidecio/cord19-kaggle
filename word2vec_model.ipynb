{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import gensim\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-27 03:49:43,448 : INFO : read 0 reviews\n",
      "2020-05-27 03:49:44,970 : INFO : read 10000 reviews\n",
      "2020-05-27 03:49:46,401 : INFO : read 20000 reviews\n",
      "2020-05-27 03:49:47,789 : INFO : read 30000 reviews\n",
      "2020-05-27 03:49:49,219 : INFO : read 40000 reviews\n",
      "2020-05-27 03:49:50,530 : INFO : read 50000 reviews\n",
      "2020-05-27 03:49:51,943 : INFO : read 60000 reviews\n",
      "2020-05-27 03:49:53,361 : INFO : read 70000 reviews\n",
      "2020-05-27 03:49:54,641 : INFO : read 80000 reviews\n",
      "2020-05-27 03:49:56,915 : INFO : read 90000 reviews\n",
      "2020-05-27 03:49:58,498 : INFO : read 100000 reviews\n",
      "2020-05-27 03:49:59,928 : INFO : read 110000 reviews\n",
      "2020-05-27 03:50:01,377 : INFO : read 120000 reviews\n",
      "2020-05-27 03:50:02,828 : INFO : read 130000 reviews\n",
      "2020-05-27 03:50:05,476 : INFO : read 140000 reviews\n",
      "2020-05-27 03:50:06,826 : INFO : read 150000 reviews\n",
      "2020-05-27 03:50:08,232 : INFO : read 160000 reviews\n",
      "2020-05-27 03:50:10,879 : INFO : read 170000 reviews\n",
      "2020-05-27 03:50:12,224 : INFO : read 180000 reviews\n",
      "2020-05-27 03:50:13,624 : INFO : read 190000 reviews\n",
      "2020-05-27 03:50:14,993 : INFO : read 200000 reviews\n",
      "2020-05-27 03:50:16,776 : INFO : read 210000 reviews\n",
      "2020-05-27 03:50:18,128 : INFO : read 220000 reviews\n",
      "2020-05-27 03:50:19,561 : INFO : read 230000 reviews\n",
      "2020-05-27 03:50:20,920 : INFO : read 240000 reviews\n",
      "2020-05-27 03:50:22,142 : INFO : read 250000 reviews\n",
      "2020-05-27 03:50:23,469 : INFO : read 260000 reviews\n",
      "2020-05-27 03:50:24,852 : INFO : read 270000 reviews\n",
      "2020-05-27 03:50:26,296 : INFO : read 280000 reviews\n",
      "2020-05-27 03:50:27,740 : INFO : read 290000 reviews\n",
      "2020-05-27 03:50:29,202 : INFO : read 300000 reviews\n",
      "2020-05-27 03:50:31,090 : INFO : read 310000 reviews\n",
      "2020-05-27 03:50:32,474 : INFO : read 320000 reviews\n",
      "2020-05-27 03:50:33,910 : INFO : read 330000 reviews\n",
      "2020-05-27 03:50:35,557 : INFO : read 340000 reviews\n",
      "2020-05-27 03:50:37,036 : INFO : read 350000 reviews\n",
      "2020-05-27 03:50:38,394 : INFO : read 360000 reviews\n",
      "2020-05-27 03:50:39,861 : INFO : read 370000 reviews\n",
      "2020-05-27 03:50:41,247 : INFO : read 380000 reviews\n",
      "2020-05-27 03:50:42,678 : INFO : read 390000 reviews\n",
      "2020-05-27 03:50:44,082 : INFO : read 400000 reviews\n",
      "2020-05-27 03:50:46,162 : INFO : read 410000 reviews\n",
      "2020-05-27 03:50:47,281 : INFO : read 420000 reviews\n",
      "2020-05-27 03:50:48,741 : INFO : read 430000 reviews\n",
      "2020-05-27 03:50:50,135 : INFO : read 440000 reviews\n",
      "2020-05-27 03:50:51,484 : INFO : read 450000 reviews\n",
      "2020-05-27 03:50:52,899 : INFO : read 460000 reviews\n",
      "2020-05-27 03:50:54,254 : INFO : read 470000 reviews\n",
      "2020-05-27 03:50:55,681 : INFO : read 480000 reviews\n",
      "2020-05-27 03:50:57,028 : INFO : read 490000 reviews\n",
      "2020-05-27 03:50:58,559 : INFO : read 500000 reviews\n",
      "2020-05-27 03:50:59,994 : INFO : read 510000 reviews\n",
      "2020-05-27 03:51:01,366 : INFO : read 520000 reviews\n",
      "2020-05-27 03:51:02,817 : INFO : read 530000 reviews\n",
      "2020-05-27 03:51:04,312 : INFO : read 540000 reviews\n",
      "2020-05-27 03:51:06,612 : INFO : read 550000 reviews\n",
      "2020-05-27 03:51:08,101 : INFO : read 560000 reviews\n",
      "2020-05-27 03:51:09,500 : INFO : read 570000 reviews\n",
      "2020-05-27 03:51:11,040 : INFO : read 580000 reviews\n",
      "2020-05-27 03:51:12,473 : INFO : read 590000 reviews\n",
      "2020-05-27 03:51:13,900 : INFO : read 600000 reviews\n",
      "2020-05-27 03:51:15,280 : INFO : read 610000 reviews\n",
      "2020-05-27 03:51:16,740 : INFO : read 620000 reviews\n",
      "2020-05-27 03:51:18,139 : INFO : read 630000 reviews\n",
      "2020-05-27 03:51:19,730 : INFO : read 640000 reviews\n",
      "2020-05-27 03:51:21,209 : INFO : read 650000 reviews\n",
      "2020-05-27 03:51:22,652 : INFO : read 660000 reviews\n",
      "2020-05-27 03:51:24,086 : INFO : read 670000 reviews\n",
      "2020-05-27 03:51:25,620 : INFO : read 680000 reviews\n",
      "2020-05-27 03:51:27,044 : INFO : read 690000 reviews\n",
      "2020-05-27 03:51:28,288 : INFO : read 700000 reviews\n",
      "2020-05-27 03:51:29,557 : INFO : read 710000 reviews\n",
      "2020-05-27 03:51:32,028 : INFO : read 720000 reviews\n",
      "2020-05-27 03:51:33,452 : INFO : read 730000 reviews\n",
      "2020-05-27 03:51:34,935 : INFO : read 740000 reviews\n",
      "2020-05-27 03:51:36,501 : INFO : read 750000 reviews\n",
      "2020-05-27 03:51:38,015 : INFO : read 760000 reviews\n",
      "2020-05-27 03:51:39,469 : INFO : read 770000 reviews\n",
      "2020-05-27 03:51:40,896 : INFO : read 780000 reviews\n",
      "2020-05-27 03:51:42,331 : INFO : read 790000 reviews\n",
      "2020-05-27 03:51:43,679 : INFO : read 800000 reviews\n",
      "2020-05-27 03:51:45,171 : INFO : read 810000 reviews\n",
      "2020-05-27 03:51:46,655 : INFO : read 820000 reviews\n",
      "2020-05-27 03:51:48,095 : INFO : read 830000 reviews\n",
      "2020-05-27 03:51:49,383 : INFO : read 840000 reviews\n",
      "2020-05-27 03:51:50,857 : INFO : read 850000 reviews\n",
      "2020-05-27 03:51:52,473 : INFO : read 860000 reviews\n",
      "2020-05-27 03:51:53,834 : INFO : read 870000 reviews\n",
      "2020-05-27 03:51:55,156 : INFO : read 880000 reviews\n",
      "2020-05-27 03:51:56,538 : INFO : read 890000 reviews\n",
      "2020-05-27 03:51:57,891 : INFO : read 900000 reviews\n",
      "2020-05-27 03:51:59,372 : INFO : read 910000 reviews\n",
      "2020-05-27 03:52:00,747 : INFO : read 920000 reviews\n",
      "2020-05-27 03:52:03,496 : INFO : read 930000 reviews\n",
      "2020-05-27 03:52:04,824 : INFO : read 940000 reviews\n",
      "2020-05-27 03:52:06,246 : INFO : read 950000 reviews\n",
      "2020-05-27 03:52:07,660 : INFO : read 960000 reviews\n",
      "2020-05-27 03:52:09,105 : INFO : read 970000 reviews\n",
      "2020-05-27 03:52:10,579 : INFO : read 980000 reviews\n",
      "2020-05-27 03:52:11,965 : INFO : read 990000 reviews\n",
      "2020-05-27 03:52:13,303 : INFO : read 1000000 reviews\n",
      "2020-05-27 03:52:14,681 : INFO : read 1010000 reviews\n",
      "2020-05-27 03:52:15,997 : INFO : read 1020000 reviews\n",
      "2020-05-27 03:52:17,368 : INFO : read 1030000 reviews\n",
      "2020-05-27 03:52:17,785 : INFO : Done reading data file\n"
     ]
    }
   ],
   "source": [
    "def read_input(input_file):\n",
    "    with open (input_file, 'r', encoding='utf8') as f:\n",
    "        for i,line in enumerate(f):\n",
    "            if (i%10000==0):\n",
    "                logging.info (\"read {0} reviews\".format (i))\n",
    "            # do some pre-processing and return a list of words for each review text\n",
    "            yield gensim.utils.simple_preprocess (line)\n",
    "            \n",
    "documents = list (read_input ('data/cord19_kaggle/custom-pdf_cord19_raw_data_v2.txt'))\n",
    "logging.info (\"Done reading data file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()   # calculate total training time\n",
    "\n",
    "model = gensim.models.Word2Vec (documents,\n",
    "                                size=200, \n",
    "                                window=10, \n",
    "                                min_count=10, \n",
    "                                sg=1, \n",
    "                                workers=7)\n",
    "model.train(documents,total_examples=len(documents),epochs=5)\n",
    "print(\"Training completed in %s seconds.\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model for convenience and prevent long training times\n",
    "model.save(\"word2vec_sg_custom-pdf_cord19_raw_data_v2_837mb.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-27 07:57:57,007 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-05-27 07:57:57,007 : INFO : built Dictionary(12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...) from 9 documents (total 29 corpus positions)\n",
      "/home/ccidecio/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `evaluate_word_pairs` (Method will be removed in 4.0.0, use self.wv.evaluate_word_pairs() instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "2020-05-27 07:57:58,793 : INFO : Pearson correlation coefficient against /home/ccidecio/anaconda3/lib/python3.7/site-packages/gensim/test/test_data/wordsim353.tsv: 0.4697\n",
      "2020-05-27 07:57:58,794 : INFO : Spearman rank-order correlation coefficient against /home/ccidecio/anaconda3/lib/python3.7/site-packages/gensim/test/test_data/wordsim353.tsv: 0.4567\n",
      "2020-05-27 07:57:58,794 : INFO : Pairs with unknown words ratio: 4.2%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((0.4697024958058125, 5.97079244331236e-20),\n",
       " SpearmanrResult(correlation=0.45671514102812144, pvalue=8.046952154950104e-19),\n",
       " 4.2492917847025495)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.test.utils import datapath\n",
    "# evaluate against wordsim-353 dataset, returns pearson and spearman coefficients\n",
    "model.evaluate_word_pairs(datapath('wordsim353.tsv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('coronaviruses', 0.7892695665359497),\n",
       " ('corona', 0.7099049687385559),\n",
       " ('cov', 0.7075159549713135),\n",
       " ('betacoronavirus', 0.6912431716918945),\n",
       " ('covs', 0.6769834756851196),\n",
       " ('coronaviral', 0.6627583503723145),\n",
       " ('coronaviridae', 0.6545959711074829),\n",
       " ('virus', 0.6510963439941406),\n",
       " ('hcov', 0.6498087644577026),\n",
       " ('sars', 0.6487791538238525)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = [\"coronavirus\"]\n",
    "model.wv.most_similar(positive=word, topn = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('coronavirus', 0.7099050283432007),\n",
       " ('coronaviruses', 0.5861067771911621),\n",
       " ('rota', 0.5791054964065552),\n",
       " ('coronaviridae', 0.5378038287162781),\n",
       " ('togavirus', 0.5368736386299133),\n",
       " ('syncytial', 0.5273870229721069),\n",
       " ('calici', 0.5220611095428467),\n",
       " ('petal', 0.5193419456481934),\n",
       " ('plcorna', 0.5183882713317871),\n",
       " ('toro', 0.5107802152633667)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = [\"corona\"]\n",
    "model.wv.most_similar(positive=word, topn = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spreading', 0.7944375276565552),\n",
       " ('spreads', 0.7183796763420105),\n",
       " ('transmission', 0.7025080919265747),\n",
       " ('dissemination', 0.6902173757553101),\n",
       " ('dispersal', 0.6286017894744873),\n",
       " ('transmitted', 0.5987448692321777),\n",
       " ('emergence', 0.5906115770339966),\n",
       " ('outbreaks', 0.5720475912094116),\n",
       " ('transmitting', 0.5716133117675781),\n",
       " ('toperson', 0.5693148374557495)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = [\"spread\"]\n",
    "model.wv.most_similar(positive=word, topn = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('diseases', 0.7756211161613464),\n",
       " ('illness', 0.6377783417701721),\n",
       " ('illnesses', 0.575861930847168),\n",
       " ('fatal', 0.566985547542572),\n",
       " ('pathology', 0.5628936290740967),\n",
       " ('debilitating', 0.5621656179428101),\n",
       " ('morbidity', 0.5568110346794128),\n",
       " ('infections', 0.5560518503189087),\n",
       " ('chronic', 0.5524715185165405),\n",
       " ('contagious', 0.5468723177909851)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = [\"disease\"]\n",
    "model.wv.most_similar(positive=word, topn = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fevers', 0.8199060559272766),\n",
       " ('myalgia', 0.795329213142395),\n",
       " ('chills', 0.7770258188247681),\n",
       " ('myalgias', 0.7716023921966553),\n",
       " ('arthralgia', 0.7509876489639282),\n",
       " ('headache', 0.7490516304969788),\n",
       " ('febrile', 0.7348302006721497),\n",
       " ('rigors', 0.713218092918396),\n",
       " ('sore', 0.7037040591239929),\n",
       " ('malaise', 0.6998950242996216)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = [\"fever\"]\n",
    "model.wv.most_similar(positive=word, topn = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pneumonias', 0.858900785446167),\n",
       " ('pneumonitis', 0.7347062826156616),\n",
       " ('lobar', 0.686647891998291),\n",
       " ('bronchiolitis', 0.6854633092880249),\n",
       " ('lrti', 0.6762850880622864),\n",
       " ('necrotizing', 0.6573134660720825),\n",
       " ('monia', 0.6445252895355225),\n",
       " ('tracheitis', 0.6438243389129639),\n",
       " ('radiologically', 0.629547655582428),\n",
       " ('atypical', 0.6254950165748596)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = [\"pneumonia\"]\n",
    "model.wv.most_similar(positive=word, topn = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cov', 0.893616795539856),\n",
       " ('mers', 0.7883569002151489),\n",
       " ('ncov', 0.7213670015335083),\n",
       " ('urbani', 0.6776567101478577),\n",
       " ('coronavirus', 0.6487791538238525),\n",
       " ('sarsassociated', 0.6247857213020325),\n",
       " ('drosten', 0.6160549521446228),\n",
       " ('scov', 0.6074730157852173),\n",
       " ('covinfected', 0.5945044755935669),\n",
       " ('ksiazek', 0.5894771814346313)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = [\"sars\"]\n",
    "model.wv.most_similar(positive=word, topn = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mers', 0.9025729894638062),\n",
       " ('sars', 0.893616795539856),\n",
       " ('emc', 0.7122606635093689),\n",
       " ('coronavirus', 0.7075159549713135),\n",
       " ('ncov', 0.7063145637512207),\n",
       " ('hcov', 0.6867600083351135),\n",
       " ('scov', 0.6807666420936584),\n",
       " ('covs', 0.6754277944564819),\n",
       " ('hku', 0.6661197543144226),\n",
       " ('urbani', 0.6451287269592285)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = [\"cov\"]\n",
    "model.wv.most_similar(positive=word, topn = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cov', 0.9025730490684509),\n",
       " ('sars', 0.7883569002151489),\n",
       " ('emc', 0.7174680233001709),\n",
       " ('dromedary', 0.677073061466217),\n",
       " ('arabia', 0.6653499603271484),\n",
       " ('saudi', 0.6644468903541565),\n",
       " ('ncov', 0.6634178161621094),\n",
       " ('camels', 0.6282460689544678),\n",
       " ('dromedaries', 0.6174394488334656),\n",
       " ('rbd', 0.6096728444099426)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = [\"mers\"]\n",
    "model.wv.most_similar(positive=word, topn = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pandemics', 0.8425107002258301),\n",
       " ('influenza', 0.7091113924980164),\n",
       " ('interpandemic', 0.7005625367164612),\n",
       " ('pdm', 0.6938039064407349),\n",
       " ('flu', 0.6925145387649536),\n",
       " ('outbreak', 0.683133602142334),\n",
       " ('uenza', 0.6830485463142395),\n",
       " ('epidemic', 0.6809103488922119),\n",
       " ('preparedness', 0.6794367432594299),\n",
       " ('epidemics', 0.6663550138473511)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = [\"pandemic\"]\n",
    "model.wv.most_similar(positive=word, topn = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ncov', 0.7544008493423462),\n",
       " ('wuhan', 0.6692982912063599),\n",
       " ('hubei', 0.6026048064231873),\n",
       " ('feb', 0.5619672536849976),\n",
       " ('sars', 0.5490542054176331),\n",
       " ('lockdown', 0.5469571352005005),\n",
       " ('ncip', 0.5465250015258789),\n",
       " ('jinyintan', 0.5342410206794739),\n",
       " ('pandemic', 0.5337203741073608),\n",
       " ('lombardy', 0.5331996083259583)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = [\"covid\"]\n",
    "model.wv.most_similar(positive=word, topn = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hubei', 0.8862814903259277),\n",
       " ('guangdong', 0.7769137024879456),\n",
       " ('guangzhou', 0.7485203742980957),\n",
       " ('ncov', 0.741247296333313),\n",
       " ('province', 0.7402205467224121),\n",
       " ('china', 0.7297977209091187),\n",
       " ('beijing', 0.72397780418396),\n",
       " ('shenzhen', 0.7019610404968262),\n",
       " ('guandong', 0.6946756839752197),\n",
       " ('huanan', 0.683606743812561)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = [\"wuhan\"]\n",
    "model.wv.most_similar(positive=word, topn = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lungs', 0.813745379447937),\n",
       " ('pulmonary', 0.7852080464363098),\n",
       " ('bronchial', 0.7084548473358154),\n",
       " ('alveolar', 0.6931854486465454),\n",
       " ('emphysema', 0.6539890170097351),\n",
       " ('airways', 0.6460864543914795),\n",
       " ('tissue', 0.6410072445869446),\n",
       " ('fibrotic', 0.6365219354629517),\n",
       " ('airway', 0.6251440644264221),\n",
       " ('bronchiolar', 0.6244347095489502)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = [\"lung\"]\n",
    "model.wv.most_similar(positive=word, topn = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# similarity between two different words\n",
    "model.wv.similarity(w1=\"recep\",w2=\"tayyip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which one is the odd one out in this list?\n",
    "model.wv.doesnt_match([\"vladimir\",\"rusya\",\"ekonomi\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveModel(model,name):\n",
    "    model.save(name)\n",
    "\n",
    "def loadModel():\n",
    "    model = Word2Vec.load('word2vec.bin')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findCosineDistance(vector1, vector2):\n",
    "    # similarity between two word vectors\n",
    "    print(\"vec1:\" + np.float64(np.linalg.norm(vector1)).astype(str) + \"| vec2:\" + np.float64(np.linalg.norm(vector2)).astype(str))\n",
    "    return np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findSimilarity(model, word1, word2):\n",
    "    # similarity between two words\n",
    "    return model.wv.similarity(w1=word1, w2=word2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printTestResults(words):\n",
    "    for i in range(0,len(words)):\n",
    "        print(\"\\n\"  + '\\033[1m' + words[i] + '\\033[0m')\n",
    "        similar_words = model.wv.most_similar(positive=words[i], topn=5)\n",
    "        for j in range(0,5):\n",
    "            print(similar_words[j])\n",
    "\n",
    "selected_words[\"coronavirus\",\"disease\"]\n",
    "# word_count = 5      # most similar n words\n",
    "print(\"window = \" + str(window_size) + \", min_count = \" + str(min_count) + \", sg = \" + str(sg) + \", hs = \" + str(hs) + \", epochs = \" + str(epochs) )\n",
    "print(\"training time: \" + str(int(training_time)) + \" seconds\")\n",
    "\n",
    "printTestResults(selected_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordVector(token):\n",
    "    #aggregate word vectors of a named entity\n",
    "    words = token.split()\n",
    "    wordVector = np.zeros(W2V_SIZE)\n",
    "    for word in words:\n",
    "        try:\n",
    "            vector = model.wv.get_vector(word)\n",
    "            wordVector = wordVector + vector\n",
    "        except KeyError:\n",
    "            print(\"KeyError: \" + word)\n",
    "\n",
    "    return wordVector\n",
    "\n",
    "for i in range(0,len(tokens)-1):\n",
    "    for j in range (i+1, len(tokens)):\n",
    "        v1 = getWordVector(tokens[i])\n",
    "        v2 = getWordVector(tokens[j])\n",
    "        similarity = findCosineDistance(v1, v2)\n",
    "        if similarity != 'nan' and similarity > SIMILARITY_THRESHOLD:\n",
    "            print(\"hey\" + \"%s\\t%s\\t%s\\n\" % (i+1, j+1, similarity))\n",
    "            file.write(\"%s\\t%s\\t%s\\n\" % (i+1, j+1, similarity))\n",
    "            #file.write(\"%s\\t%s\\t%s\\n\"% (tokens[i], tokens[j], similarity))\n",
    "\n",
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
